{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmk2uLqMlp/BLFTjjCNBFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam4988/Assignment/blob/main/weight%20initialization%20techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is the vanishing gradient problem in deep neural networks? How does it affect training?\n",
        "The vanishing gradient problem occurs when gradients of the loss function become extremely small during backpropagation, especially in deep networks. This happens because gradients are computed using the chain rule, and repeated multiplication of small derivatives (e.g., from activation functions like sigmoid) causes the gradients to shrink exponentially. It affects training by slowing or halting weight updates in earlier layers, leading to poor model performance or failure to converge."
      ],
      "metadata": {
        "id": "7VSmii_3wyCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Explain how Xavier initialization addresses the vanishing gradient problem.\n",
        "Xavier (Glorot) initialization sets weights using a uniform or normal distribution scaled by\n",
        "\n",
        "1/√\n",
        "n\n",
        "in\n",
        "+\n",
        "n\n",
        "out,\n",
        "\n",
        "\n",
        "\n",
        "where\n",
        "n\n",
        "in\n",
        "and\n",
        "n\n",
        "out\n",
        "are the number of input and output neurons for a layer. This scaling ensures the variance of activations remains consistent across layers, preventing gradients from vanishing or exploding during backpropagation."
      ],
      "metadata": {
        "id": "YU30drCew0Na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What are some common activation functions that are prone to causing vanishing gradients?\n",
        "Sigmoid and hyperbolic tangent (tanh) are prone to vanishing gradients because their derivatives saturate (e.g., sigmoid derivatives peak at 0.25 and approach zero for large inputs). ReLU can also cause vanishing gradients in dead neurons (where outputs are zero and gradients remain zero)."
      ],
      "metadata": {
        "id": "7UIVsOWkxua4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Define the exploding gradient problem in deep neural networks. How does it impact training?\n",
        "The exploding gradient problem occurs when gradients grow excessively large during backpropagation, often due to repeated multiplication of large weight values. This causes unstable weight updates, leading to numerical overflow (NaN errors), oscillating loss values, or divergence during training."
      ],
      "metadata": {
        "id": "8--HyiVmxz3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. What is the role of proper weight initialization in training deep neural networks?\n",
        "Proper weight initialization ensures that initial weights are neither too small (causing vanishing gradients) nor too large (causing exploding gradients). It helps stabilize training, accelerates convergence, and improves the model’s ability to learn meaningful patterns."
      ],
      "metadata": {
        "id": "QajUOc5Zw8FC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Explain the concept of batch normalization and its impact on weight initialization techniques.\n",
        "Batch normalization (BN) normalizes layer outputs by adjusting them to have zero mean and unit variance. This reduces internal covariate shift, stabilizes training, and allows the use of higher learning rates. BN also diminishes the sensitivity of models to weight initialization, making techniques like Xavier or He initialization less critical (though still beneficial)."
      ],
      "metadata": {
        "id": "iaXlxvETx7ZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Implement He initialization in Python using TensorFlow or PyTorch.\n",
        "PyTorch Implementation:"
      ],
      "metadata": {
        "id": "wt3Ad2QAx-yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Example layer\n",
        "layer = nn.Linear(in_features=100, out_features=200)\n",
        "nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo1V3nnRyEFV",
        "outputId": "f27b6915-9ce0-4b48-eacd-402ed02360bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.1807, -0.1620,  0.0659,  ...,  0.0747,  0.0288,  0.1194],\n",
              "        [-0.0544,  0.1578,  0.0132,  ...,  0.0759, -0.1646, -0.1379],\n",
              "        [ 0.2130, -0.0640,  0.1729,  ..., -0.1020,  0.1606, -0.0015],\n",
              "        ...,\n",
              "        [ 0.0266, -0.2593, -0.1424,  ...,  0.0112,  0.2764,  0.0646],\n",
              "        [ 0.0182,  0.0491, -0.1955,  ...,  0.1235,  0.1049, -0.0212],\n",
              "        [ 0.0745, -0.0042,  0.0144,  ..., -0.2044, -0.1796,  0.0162]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AuT8P-Xaww6c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Example layer\n",
        "initializer = tf.keras.initializers.HeNormal()\n",
        "layer = tf.keras.layers.Dense(units=200, kernel_initializer=initializer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "He initialization scales weights by\n",
        "√2/n in ,where\n",
        "n\n",
        "in\n",
        "​ is the number of input units. It is designed for ReLU-like activations."
      ],
      "metadata": {
        "id": "FXD8Yx52yXrP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VnHZSrPL100t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}