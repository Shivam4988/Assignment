{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPThARweJSI9dfDhHZNjYWS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam4988/Assignment/blob/main/motion%2C%20flow%20of%20optics%20and%20motion%20vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Define motion estimation in computer vision and discuss its importance in various applications\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PHwh4uByYboO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Motion estimation in computer vision refers to the process of determining the movement of objects or pixels between consecutive frames in a video sequence. It involves calculating the displacement vectors that represent how objects transition from one frame to another. Techniques like block matching, optical flow, and feature tracking are commonly used for this purpose.\n",
        "\n",
        "**Importance in applications:**\n",
        "\n",
        "Video Compression: Reduces redundancy by encoding motion vectors instead of full frames (e.g., in MPEG, H.264).\n",
        "\n",
        "Object Tracking: Enables surveillance systems and autonomous vehicles to follow moving objects.\n",
        "\n",
        "Action Recognition: Helps identify human activities in sports analytics or security systems.\n",
        "\n",
        "Robotics: Assists robots in navigating dynamic environments by understanding scene motion."
      ],
      "metadata": {
        "id": "hi6DBiMaYkPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Discuss the challenges faced in motion estimation, particularly in the presence of occlusions and complex scene dynamics. Propose potential solutions to address these challenges."
      ],
      "metadata": {
        "id": "WG0sAcS3Yo3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenges:**\n",
        "\n",
        "Occlusions: Objects may temporarily hide others, causing incomplete or erroneous motion data.\n",
        "\n",
        "Complex Scene Dynamics: Rapid motion, lighting changes, or non-rigid deformations (e.g., water, cloth) complicate motion tracking.\n",
        "\n",
        "Noise and Ambiguity: Low-resolution footage or textureless regions lead to unreliable estimates.\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "Temporal Consistency: Use multi-frame analysis to predict motion during occlusions.\n",
        "\n",
        "Depth Sensors: Integrate LiDAR or stereo cameras to resolve ambiguities in 3D scenes.\n",
        "\n",
        "Machine Learning: Train models (e.g., CNNs) to infer motion in occluded regions using contextual data.\n",
        "\n",
        "Hybrid Algorithms: Combine optical flow with feature matching for robustness in dynamic scenes."
      ],
      "metadata": {
        "id": "UKXnJLTeYxnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the concept of optical flow and its role in motion estimation. Discuss common optical flow algorithms and their applications."
      ],
      "metadata": {
        "id": "Fu__aWQVY3p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optical Flow:**\n",
        "\n",
        "Optical flow is the pattern of apparent motion of objects, surfaces, or edges in a visual scene caused by the relative motion between an observer (e.g., a camera) and the scene. It is represented as a 2D vector field, where each vector indicates the direction and speed of pixel movement between frames.\n",
        "\n",
        "Role in Motion Estimation:\n",
        "It quantifies pixel-level motion, enabling precise tracking and analysis of object trajectories.\n",
        "\n",
        "**Common Algorithms:**\n",
        "\n",
        "Lucas-Kanade: Efficient for sparse flow, assumes small displacements in local regions (used in robotics).\n",
        "\n",
        "Horn-Schunck: Computes dense flow globally, suitable for smooth motion (e.g., fluid dynamics).\n",
        "\n",
        "Farnebäck’s Algorithm: Uses polynomial expansion for dense flow estimation (applications in video stabilization).\n",
        "\n",
        "Deep Learning-Based (FlowNet, RAFT): Leverage neural networks for accurate flow prediction in complex scenes.\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "Motion segmentation, video interpolation, and autonomous navigation.\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "**Code Example (Lucas-Kanade Optical Flow using OpenCV):**"
      ],
      "metadata": {
        "id": "v1u21wUIY-XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load video and initialize parameters\n",
        "cap = cv2.VideoCapture(\"input_video.mp4\")\n",
        "\n",
        "# Check if video opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error opening video file\")\n",
        "    exit() # Exit if video cannot be opened\n",
        "\n",
        "ret, prev_frame = cap.read()\n",
        "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Define Shi-Tomasi corner detection parameters\n",
        "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
        "p0 = cv2.goodFeaturesToTrack(prev_gray, mask=None, **feature_params)\n",
        "\n",
        "# Lucas-Kanade parameters\n",
        "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate optical flow\n",
        "    p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, frame_gray, p0, None, **lk_params)\n",
        "\n",
        "    # Visualize flow vectors\n",
        "    good_new = p1[st == 1]\n",
        "    good_old = p0[st == 1]\n",
        "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
        "        a, b = new.ravel()\n",
        "        c, d = old.ravel()\n",
        "        frame = cv2.line(frame, (int(a), int(b)), (int(c), int(d)), (0, 255, 0), 2)\n",
        "        frame = cv2.circle(frame, (int(a), int(b)), 5, (0, 0, 255), -1)\n",
        "\n",
        "    cv2.imshow(\"Optical Flow\", frame)\n",
        "    if cv2.waitKey(30) & 0xFF == 27:\n",
        "        break\n",
        "\n",
        "    prev_gray = frame_gray.copy()\n",
        "    p0 = good_new.reshape(-1, 1, 2)\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "HCAD93ppaPS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Define optical flow and explain its significance in computer vision applications."
      ],
      "metadata": {
        "id": "ERqs_huXaTca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition:**\n",
        "\n",
        "Optical flow is the perceived motion of visual elements (pixels) between consecutive video frames, resulting from relative movement between the camera and objects in the scene.\n",
        "\n",
        "**Significance:**\n",
        "\n",
        "Motion Analysis: Enables understanding of object speed, direction, and behavior (e.g., traffic monitoring).\n",
        "\n",
        "Video Processing: Critical for tasks like stabilization, frame interpolation, and slow-motion rendering.\n",
        "\n",
        "Human-Computer Interaction: Powers gesture recognition systems and augmented reality applications.\n",
        "\n",
        "Autonomous Systems: Helps drones and self-driving cars avoid obstacles by analyzing real-time motion.\n",
        "\n",
        "**Code Example (Dense Optical Flow with Farnebäck’s Algorithm):**"
      ],
      "metadata": {
        "id": "t0i3BtOLacLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "cap = cv2.VideoCapture(\"input_video.mp4\")\n",
        "\n",
        "# Check if video opened successfully\n",
        "if not cap.isOpened():\n",
        "    raise FileNotFoundError(\"Could not open video file. Check the path or codec support.\")\n",
        "\n",
        "ret, prev_frame = cap.read()\n",
        "\n",
        "# Ensure the first frame is valid\n",
        "if not ret:\n",
        "    print(\"Failed to read the first frame. Exiting.\")\n",
        "    cap.release()\n",
        "    exit()\n",
        "\n",
        "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break  # Exit loop if frame reading fails\n",
        "\n",
        "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate dense optical flow\n",
        "    flow = cv2.calcOpticalFlowFarneback(prev_gray, frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "    # Visualization code remains the same\n",
        "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    hsv = np.zeros_like(frame)\n",
        "    hsv[..., 1] = 255\n",
        "    hsv[..., 0] = angle * 180 / np.pi / 2\n",
        "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    cv2.imshow(\"Dense Optical Flow\", bgr)\n",
        "    if cv2.waitKey(30) & 0xFF == 27:\n",
        "        break\n",
        "\n",
        "    prev_gray = frame_gray.copy()\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "-Mh-wGGbbDr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Describe the concept of motion vectors in video compression and discuss their role in reducing redundancy."
      ],
      "metadata": {
        "id": "_71lsA3YbGPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Motion Vectors:**\n",
        "\n",
        "Motion vectors are 2D vectors that indicate the displacement of a block of pixels from one frame to another in a video sequence. They are a core component of inter-frame compression techniques.\n",
        "\n",
        "**Role in Reducing Redundancy:**\n",
        "\n",
        "Inter-Frame Prediction: Instead of storing all pixel data for each frame, codecs like MPEG or H.264 use motion vectors to reference similar blocks in previous or future frames (prediction).\n",
        "\n",
        "Residual Encoding: Only the difference (residual) between the predicted block and the actual block is encoded, minimizing data duplication.\n",
        "\n",
        "Efficiency: This approach drastically reduces bitrate while maintaining visual quality, enabling efficient streaming and storage.\n",
        "\n",
        "Example: In a video of a moving car, the background remains static. Motion vectors track the car’s movement, while the unchanged background is reused from earlier frames."
      ],
      "metadata": {
        "id": "c8qc70pdbT35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def block_matching(prev_frame, curr_frame, block_size=16, search_range=8):\n",
        "    height, width = prev_frame.shape\n",
        "    motion_vectors = []\n",
        "\n",
        "    for y in range(0, height - block_size, block_size):\n",
        "        for x in range(0, width - block_size, block_size):\n",
        "            min_sad = float('inf')\n",
        "            best_dx, best_dy = 0, 0\n",
        "\n",
        "            # Extract current block\n",
        "            block = curr_frame[y:y+block_size, x:x+block_size]\n",
        "\n",
        "            # Search in previous frame\n",
        "            for dy in range(-search_range, search_range+1):\n",
        "                for dx in range(-search_range, search_range+1):\n",
        "                    yy = y + dy\n",
        "                    xx = x + dx\n",
        "                    if yy < 0 or yy + block_size > height or xx < 0 or xx + block_size > width:\n",
        "                        continue\n",
        "                    prev_block = prev_frame[yy:yy+block_size, xx:xx+block_size]\n",
        "                    sad = np.sum(np.abs(block - prev_block))\n",
        "\n",
        "                    if sad < min_sad:\n",
        "                        min_sad = sad\n",
        "                        best_dx, best_dy = dx, dy\n",
        "\n",
        "            motion_vectors.append((best_dx, best_dy))\n",
        "\n",
        "    return motion_vectors\n",
        "\n",
        "# Example usage\n",
        "prev_frame = cv2.imread(\"frame1.png\", cv2.IMREAD_GRAYSCALE)\n",
        "curr_frame = cv2.imread(\"frame2.png\", cv2.IMREAD_GRAYSCALE)\n",
        "motion_vectors = block_matching(prev_frame, curr_frame, block_size=16, search_range=8)\n",
        "print(f\"Motion Vectors: {motion_vectors[:5]}...\")  # Display first 5 vectors"
      ],
      "metadata": {
        "id": "EfQW1v7sbh83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P1fElbr5blaV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c-92bxk1blAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}