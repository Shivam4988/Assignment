{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPGfO9zKHKU+tPWGavqb49",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam4988/Assignment/blob/main/ML_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is clustering in machine learning?\n",
        "\n",
        "Clustering is an unsupervised learning technique that groups similar data points together based on their features. The goal is to discover inherent structures within the data without prior knowledge of labels or categories."
      ],
      "metadata": {
        "id": "0_i_sDk7xfl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Explain the difference between supervised and unsupervised clustering.\n",
        "\n",
        "Clustering is inherently unsupervised. The term \"supervised clustering\" is non-standard but may refer to classification tasks where labeled data is used. In unsupervised clustering, data is grouped without labels, while supervised methods (e.g., classification) use labeled data to train models.\n",
        "\n"
      ],
      "metadata": {
        "id": "XunimnEhxlqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What are the key applications of clustering algorithms?\n",
        "\n",
        "Customer segmentation\n",
        "\n",
        "Image compression\n",
        "\n",
        "Document clustering\n",
        "\n",
        "Anomaly detection\n",
        "\n",
        "Market basket analysis"
      ],
      "metadata": {
        "id": "iSKh25Ppxzyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Describe the K-means clustering algorithm.\n",
        "\n",
        "Choose K initial centroids.\n",
        "\n",
        "Assign each data point to the nearest centroid.\n",
        "\n",
        "Recalculate centroids as the mean of assigned points.\n",
        "\n",
        "Repeat steps 2–3 until convergence (no further changes)."
      ],
      "metadata": {
        "id": "OX-IbgFSyCsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 2], [5, 8], [1.5, 1.8], [8, 8], [1, 0.6], [9, 11]])\n",
        "\n",
        "# Initialize and fit K-means\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "\n",
        "print(\"Cluster centers:\", kmeans.cluster_centers_)\n",
        "print(\"Labels:\", kmeans.labels_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS8FR9QsxzSA",
        "outputId": "fe21f215-36a2-49f1-cdea-82bfc40e5591"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster centers: [[1.16666667 1.46666667]\n",
            " [7.33333333 9.        ]]\n",
            "Labels: [0 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. What are the main advantages and disadvantages of K-means clustering?\n",
        "\n",
        "Advantages: Simple, efficient, works well with spherical clusters.\n",
        "\n",
        "Disadvantages: Sensitive to initial centroids, struggles with non-convex clusters, assumes equal cluster sizes."
      ],
      "metadata": {
        "id": "igctRHqPyXAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. How does hierarchical clustering work?\n",
        "It builds a hierarchy of clusters using either:\n",
        "\n",
        "Agglomerative: Bottom-up approach, merging closest clusters iteratively.\n",
        "\n",
        "Divisive: Top-down approach, splitting clusters recursively."
      ],
      "metadata": {
        "id": "MLMQZp6jymJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. What are the different linkage criteria used in hierarchical clustering?\n",
        "\n",
        "Single linkage (minimum distance).\n",
        "\n",
        "Complete linkage (maximum distance).\n",
        "\n",
        "Average linkage (average distance).\n",
        "\n",
        "Ward’s method (minimizes variance)."
      ],
      "metadata": {
        "id": "N0e6HZxNyoOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Explain the concept of DBSCAN clustering.\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) identifies dense regions separated by sparse regions. Points are classified as:\n",
        "\n",
        "Core points: At least min_samples within eps radius.\n",
        "\n",
        "Border points: Near core points but lack sufficient neighbors.\n",
        "\n",
        "Noise points: Neither core nor border."
      ],
      "metadata": {
        "id": "CO3G_imNyr54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])\n",
        "\n",
        "# Fit DBSCAN\n",
        "dbscan = DBSCAN(eps=3, min_samples=2)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "print(\"Labels (Noise points labeled as -1):\", labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKj6L7Yqywlf",
        "outputId": "83a4c1bb-110e-40eb-cb12-99e0029ad8f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels (Noise points labeled as -1): [ 0  0  0  1  1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. What are the parameters involved in DBSCAN clustering?\n",
        "\n",
        "eps: Radius defining neighborhood.\n",
        "\n",
        "min_samples: Minimum points required to form a dense region."
      ],
      "metadata": {
        "id": "prV-WugFy2mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Describe the process of evaluating clustering algorithms.\n",
        "Use internal metrics (e.g., silhouette score, Davies-Bouldin index), external metrics (if labels exist, e.g., adjusted Rand index), or visual assessment (e.g., dendrograms, scatter plots)."
      ],
      "metadata": {
        "id": "4anWcuAey-Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. What is the silhouette score, and how is it calculated?\n",
        "It measures how well a point fits its cluster. For a point i:\n",
        "\n",
        "s\n",
        "(\n",
        "i\n",
        ")\n",
        "=\n",
        "b\n",
        "(\n",
        "i\n",
        ")\n",
        "−\n",
        "a\n",
        "(\n",
        "i\n",
        ")\n",
        "max\n",
        "⁡\n",
        "(\n",
        "a\n",
        "(\n",
        "i\n",
        ")\n",
        ",\n",
        "b\n",
        "(\n",
        "i\n",
        ")\n",
        "/\n",
        "max(a(i),b(i))\n",
        "b(i)−a(i))\n",
        "​\n",
        "\n",
        "where\n",
        "a\n",
        "(\n",
        "i\n",
        ")\n",
        "a(i) = mean intra-cluster distance,\n",
        "b\n",
        "(\n",
        "i\n",
        ")\n",
        "b(i) = mean distance to the nearest neighboring cluster."
      ],
      "metadata": {
        "id": "v2fFiNS4zDpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Discuss the challenges of clustering high-dimensional data.\n",
        "\n",
        "Curse of dimensionality: Distance metrics lose meaning.\n",
        "\n",
        "Increased sparsity.\n",
        "\n",
        "Noise dominates.\n",
        "Solutions: Dimensionality reduction (PCA, t-SNE)."
      ],
      "metadata": {
        "id": "GIPfRvAZz0i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Explain the concept of density-based clustering.\n",
        "Clusters are defined as regions of higher density compared to their surroundings. Examples: DBSCAN, OPTICS."
      ],
      "metadata": {
        "id": "jHo5NQCmz7AV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. How does Gaussian Mixture Model (GMM) clustering differ from K-means?\n",
        "GMM assumes data is generated from a mixture of Gaussian distributions. It allows soft assignments (probabilistic) and captures elliptical clusters, unlike K-means (hard assignments, spherical clusters)."
      ],
      "metadata": {
        "id": "C_0na7u0z-lY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. What are the limitations of traditional clustering algorithms?\n",
        "\n",
        "Assume specific cluster shapes (e.g., spherical).\n",
        "\n",
        "Struggle with noise and outliers.\n",
        "\n",
        "Require predefined parameters (e.g., K in K-means)."
      ],
      "metadata": {
        "id": "3DS3ffNW0D8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Discuss the applications of spectral clustering.\n",
        "\n",
        "Image segmentation.\n",
        "\n",
        "Social network analysis.\n",
        "\n",
        "Bioinformatics (gene expression analysis)."
      ],
      "metadata": {
        "id": "GBaD1RZI0Kik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. Explain the concept of affinity propagation.\n",
        "A clustering algorithm that identifies exemplars (representative points) by exchanging messages between data points. It does not require specifying the number of clusters upfront.\n",
        "\n"
      ],
      "metadata": {
        "id": "kHeO4jJX0NvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. How do you handle categorical variables in clustering?\n",
        "\n",
        "Use algorithms like K-modes.\n",
        "\n",
        "Apply distance metrics for categorical data (e.g., Hamming distance).\n",
        "\n",
        "Encode variables (e.g., one-hot encoding)."
      ],
      "metadata": {
        "id": "mbYHaBVh0SAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 19. Describe the elbow method for determining the optimal number of clusters.\n",
        "Plot the within-cluster sum of squares (WCSS) against the number of clusters (K). The \"elbow\" point (where WCSS decline slows) indicates optimal K."
      ],
      "metadata": {
        "id": "WOo8cmRw0Wjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20. What are some emerging trends in clustering research?\n",
        "\n",
        "Deep clustering (e.g., autoencoders).\n",
        "\n",
        "Subspace clustering.\n",
        "\n",
        "Integration with reinforcement learning."
      ],
      "metadata": {
        "id": "U6NYvA2d0acm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 21. What is anomaly detection, and why is it important?\n",
        "Anomaly detection identifies rare events or outliers. It is critical for fraud detection, system health monitoring, and quality control."
      ],
      "metadata": {
        "id": "Ttg9v0nf0eHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 22. Discuss the types of anomalies encountered in anomaly detection.\n",
        "\n",
        "Point anomalies: Single unusual instances.\n",
        "\n",
        "Contextual anomalies: Abnormal in specific contexts (e.g., temperature spikes in winter).\n",
        "\n",
        "Collective anomalies: A group of instances deviating as a whole."
      ],
      "metadata": {
        "id": "z8qQia8D0iFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 23. Explain the difference between supervised and unsupervised anomaly detection techniques.\n",
        "\n",
        "Supervised: Uses labeled data (normal vs. anomalies) to train a classifier.\n",
        "\n",
        "Unsupervised: Assumes anomalies are rare and distinct, requiring no labels."
      ],
      "metadata": {
        "id": "TvNyhSVF0lZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 24. Describe the Isolation Forest algorithm for anomaly detection.\n",
        "Isolation Forest isolates anomalies by recursively partitioning data using random splits. Anomalies require fewer splits to isolate, resulting in shorter path lengths in decision trees.\n",
        "\n"
      ],
      "metadata": {
        "id": "XKHeHtWd0pNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Sample data\n",
        "X = [[0.5], [1.2], [3.4], [5.6], [120]]  # 120 is an outlier\n",
        "\n",
        "# Fit model\n",
        "clf = IsolationForest(contamination=0.1)\n",
        "clf.fit(X)\n",
        "\n",
        "print(\"Predictions (-1 = anomaly):\", clf.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IaA16ye0v13",
        "outputId": "494fb02a-83dc-45e4-b2c0-fd198f2c2015"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions (-1 = anomaly): [ 1  1  1  1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 25. How does One-Class SVM work in anomaly detection?\n",
        "It learns a decision boundary around normal data points in a high-dimensional space. Points outside the boundary are classified as anomalies.\n",
        "\n"
      ],
      "metadata": {
        "id": "g3iC4ozD0z17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 26. Discuss the challenges of anomaly detection in high-dimensional data.\n",
        "\n",
        "Increased computational complexity.\n",
        "\n",
        "Irrelevant features masking anomalies.\n",
        "\n",
        "Sparsity reduces detection accuracy."
      ],
      "metadata": {
        "id": "oA-2YF1805WE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 27. Explain the concept of novelty detection.\n",
        "Identifying new/unseen patterns that were not present in the training data. Differs from anomaly detection, which focuses on known anomalies."
      ],
      "metadata": {
        "id": "GjWxKiLC09gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 28. What are some real-world applications of anomaly detection?\n",
        "\n",
        "Credit card fraud detection.\n",
        "\n",
        "Network intrusion detection.\n",
        "\n",
        "Industrial defect detection."
      ],
      "metadata": {
        "id": "ESJN-9pP1Amb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 29. Describe the Local Outlier Factor (LOF) algorithm.\n",
        "LOF measures the local density deviation of a point compared to its neighbors. A score >> 1 indicates an outlier (lower density than neighbors)."
      ],
      "metadata": {
        "id": "lnMTEcTa1D-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 30. How do you evaluate the performance of an anomaly detection model?\n",
        "\n",
        "Metrics: Precision, recall, F1-score, ROC-AUC.\n",
        "\n",
        "Challenges: Imbalanced data, lack of labeled anomalies."
      ],
      "metadata": {
        "id": "DedJrSkT1HVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 31. Discuss the role of feature engineering in anomaly detection.\n",
        "Feature engineering creates relevant features to highlight anomalies (e.g., time-based aggregates, domain-specific transformations)."
      ],
      "metadata": {
        "id": "RWi3T8E01Lw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 32. What are the limitations of traditional anomaly detection methods?\n",
        "\n",
        "Assume specific data distributions.\n",
        "\n",
        "Struggle with high-dimensional data.\n",
        "\n",
        "Limited adaptability to dynamic environments."
      ],
      "metadata": {
        "id": "kMCwKbym1PAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 33. Explain the concept of ensemble methods in anomaly detection.\n",
        "Combining multiple models (e.g., Isolation Forest, LOF) to improve robustness and accuracy. Example: Voting systems or score averaging."
      ],
      "metadata": {
        "id": "pEXDI_pK1U08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 34. How does autoencoder-based anomaly detection work?\n",
        "An autoencoder is trained to reconstruct normal data. High reconstruction error indicates anomalies."
      ],
      "metadata": {
        "id": "XCgVpN7w1Yqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Build autoencoder\n",
        "encoder = tf.keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\", input_shape=(10,)),\n",
        "    layers.Dense(8, activation=\"relu\")\n",
        "])\n",
        "\n",
        "decoder = tf.keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\", input_shape=(8,)),\n",
        "    layers.Dense(10, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "autoencoder = tf.keras.Sequential([encoder, decoder])\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train on normal data and compute reconstruction error\n",
        "# reconstruction_error = tf.reduce_mean(tf.square(X_test - autoencoder.predict(X_test)), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJX8-c_h1hbi",
        "outputId": "ceb0b36f-fc96-4243-bc76-f70373c53270"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 35. What are some approaches for handling imbalanced data in anomaly detection?\n",
        "\n",
        "Resampling (oversampling anomalies, undersampling normal data).\n",
        "\n",
        "Adjusting class weights in models.\n",
        "\n",
        "Using anomaly score thresholds."
      ],
      "metadata": {
        "id": "MT3KRV7o1clx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 36. Describe the concept of semi-supervised anomaly detection.\n",
        "Uses a small amount of labeled normal data and large unlabeled data. Models learn normal patterns and flag deviations."
      ],
      "metadata": {
        "id": "LUAutmCW1tVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 37. Discuss the trade-offs between false positives and false negatives in anomaly detection.\n",
        "\n",
        "False positives: Normal instances flagged as anomalies (costly in low-tolerance systems).\n",
        "\n",
        "False negatives: Missed anomalies (risky in critical applications).\n",
        "The balance depends on domain requirements."
      ],
      "metadata": {
        "id": "7pUQlfQl1ysI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 38. How do you interpret the results of an anomaly detection model?\n",
        "\n",
        "Analyze anomaly scores.\n",
        "\n",
        "Investigate feature contributions (e.g., SHAP values).\n",
        "\n",
        "Validate with domain experts."
      ],
      "metadata": {
        "id": "YkWnLIhS12ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 39. What are some open research challenges in anomaly detection?\n",
        "\n",
        "Real-time detection in streaming data.\n",
        "\n",
        "Explainability of models.\n",
        "\n",
        "Adapting to evolving anomaly patterns."
      ],
      "metadata": {
        "id": "_YN1nnbp15q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 40. Explain the concept of contextual anomaly detection.\n",
        "Identifies anomalies within specific contexts (e.g., time, location). For example, a sudden spike in sales at an unusual hour."
      ],
      "metadata": {
        "id": "mkGjKA2F18kL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 41. What is time series analysis, and what are its key components?\n",
        "Analysis of time-ordered data. Components:\n",
        "\n",
        "Trend: Long-term direction.\n",
        "\n",
        "Seasonality: Periodic fluctuations.\n",
        "\n",
        "Residual: Irregular noise."
      ],
      "metadata": {
        "id": "k62l2bbr1_up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 42. Discuss the difference between univariate and multivariate time series analysis.\n",
        "\n",
        "Univariate: Single variable tracked over time.\n",
        "\n",
        "Multivariate: Multiple variables tracked, considering interdependencies."
      ],
      "metadata": {
        "id": "U4ONDJzU2Ebu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 43. Describe the process of time series decomposition.\n",
        "Separates a time series into:\n",
        "\n",
        "Trend component.\n",
        "\n",
        "Seasonal component.\n",
        "\n",
        "Residual component.\n",
        "Methods: Additive or multiplicative decomposition."
      ],
      "metadata": {
        "id": "zMACyGqe2Hsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 44. What are the main components of a time series decomposition?\n",
        "\n",
        "Trend\n",
        "\n",
        "Seasonality\n",
        "\n",
        "Cyclical (long-term fluctuations)\n",
        "\n",
        "Irregular (random noise)"
      ],
      "metadata": {
        "id": "S6vzJS1v2Mkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 45. Explain the concept of stationarity in time series data.\n",
        "A stationary series has constant mean, variance, and autocorrelation over time. Required for models like ARIMA."
      ],
      "metadata": {
        "id": "XDs6ZNVK2OOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 46. How do you test for stationarity in a time series?\n",
        "\n",
        "Augmented Dickey-Fuller (ADF) test: Null hypothesis = non-stationary. Reject if p-value < 0.05.\n",
        "\n",
        "Visual inspection (rolling mean/variance)."
      ],
      "metadata": {
        "id": "VkhrXW2s2Rhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 47. Discuss the autoregressive integrated moving average (ARIMA) model.\n",
        "\n",
        "ARIMA combines autoregressive (AR), differencing (I), and moving average (MA) components.\n"
      ],
      "metadata": {
        "id": "fMYx2dBl2UeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Sample time series data\n",
        "data = [10, 20, 30, 40, 50, 60, 70]\n",
        "\n",
        "# Fit ARIMA(p=1, d=1, q=1)\n",
        "model = ARIMA(data, order=(1, 1, 1))\n",
        "results = model.fit()\n",
        "\n",
        "# Forecast\n",
        "print(\"Forecast:\", results.forecast(steps=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed2qo8wC2X8-",
        "outputId": "f7f84d1d-e6eb-4556-a607-126f471f865d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
            "  warn('Non-stationary starting autoregressive parameters'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forecast: [79.99993289 89.99975073 99.99945354]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 48. What are the parameters of the ARIMA model?\n",
        "\n",
        "p: Autoregressive order.\n",
        "\n",
        "d: Differencing order.\n",
        "\n",
        "q: Moving average order."
      ],
      "metadata": {
        "id": "Q-D6llK42ivv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 49. Describe the seasonal autoregressive integrated moving average (SARIMA) model.\n",
        "SARIMA extends ARIMA with seasonal terms: SARIMA(p,d,q)(P,D,Q,m), where m = seasonal period."
      ],
      "metadata": {
        "id": "P9O4yN0i2ng3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 50. How do you choose the appropriate lag order in an ARIMA model?\n",
        "Use ACF (identifies q) and PACF (identifies p) plots. The lag where ACF/PACF cuts off indicates the order."
      ],
      "metadata": {
        "id": "C79L_hVh2qOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 51. Explain the concept of differencing in time series analysis.\n",
        "Differencing subtracts the previous observation (\n",
        "y\n",
        "t\n",
        "−\n",
        "y\n",
        "t\n",
        "−\n",
        "1\n",
        "y\n",
        "t\n",
        "​\n",
        " −y\n",
        "t−1\n",
        "​\n",
        " ) to stabilize the mean and remove trends."
      ],
      "metadata": {
        "id": "PTgJ74e62tQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 52. What is the Box-Jenkins methodology?\n",
        "A three-step approach for ARIMA modeling:\n",
        "\n",
        "Identification: Determine p, d, q using ACF/PACF.\n",
        "\n",
        "Estimation: Fit the model.\n",
        "\n",
        "Diagnostics: Validate residuals (e.g., Ljung-Box test)."
      ],
      "metadata": {
        "id": "2TRFk61H2xKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 53. Discuss the role of ACF and PACF plots in identifying ARIMA parameters.\n",
        "\n",
        "ACF: Helps identify q (MA term) by showing correlation at lags.\n",
        "\n",
        "PACF: Helps identify p (AR term) by showing partial correlations."
      ],
      "metadata": {
        "id": "pY9hUyXe20aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 54. How do you handle missing values in time series data?\n",
        "\n",
        "Interpolation (linear, spline).\n",
        "\n",
        "Forward/backward filling.\n",
        "\n",
        "Model-based imputation (e.g., Kalman filter)."
      ],
      "metadata": {
        "id": "aUxHxsRl23jO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 55. Describe the concept of exponential smoothing.\n",
        "\n",
        "Exponential smoothing assigns decaying weights to past observations."
      ],
      "metadata": {
        "id": "3-bF0TWX29w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
        "\n",
        "# Sample data\n",
        "data = [10.2, 11.5, 12.3, 13.6, 14.7]\n",
        "\n",
        "# Fit model\n",
        "model = SimpleExpSmoothing(data)\n",
        "fit = model.fit(smoothing_level=0.5)\n",
        "\n",
        "# Forecast\n",
        "print(\"Next value:\", fit.forecast(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk3Nt4WU3Tub",
        "outputId": "06e90d36-5e54-4aee-b69b-b5a1b2b2d720"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next value: [13.64375]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/util/_decorators.py:213: EstimationWarning: Model has no free parameters to estimate. Set optimized=False to suppress this warning\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 56. What is the Holt-Winters method, and when is it used?\n",
        "Extends exponential smoothing to capture trend and seasonality. Used for data with both components. Types: Additive and multiplicative."
      ],
      "metadata": {
        "id": "OJOTBf_-3XD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 57. Discuss the challenges of forecasting long-term trends in time series data.\n",
        "\n",
        "Uncertainty accumulates over time.\n",
        "\n",
        "Structural breaks (e.g., economic crises).\n",
        "\n",
        "External factors unaccounted in the model."
      ],
      "metadata": {
        "id": "bg5BanWR3fQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 58. Explain the concept of seasonality in time series analysis.\n",
        "Regular, repeating patterns tied to time intervals (e.g., daily, monthly). Example: Holiday sales spikes."
      ],
      "metadata": {
        "id": "PszvDw1Q3luK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 59. How do you evaluate the performance of a time series forecasting model?\n",
        "Metrics:\n",
        "\n",
        "MAE (Mean Absolute Error).\n",
        "\n",
        "RMSE (Root Mean Squared Error).\n",
        "\n",
        "MAPE (Mean Absolute Percentage Error)."
      ],
      "metadata": {
        "id": "c5CRZC083qFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Actual vs predicted values\n",
        "y_true = [3, 5, 7, 9]\n",
        "y_pred = [2.8, 5.2, 7.1, 8.5]\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2djMv3U3v-B",
        "outputId": "30b36f61-edfb-416b-ac85-ef9f558fd1c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.25, RMSE: 0.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 60. What are some advanced techniques for time series forecasting?\n",
        "\n",
        "LSTM networks (for capturing long-term dependencies).\n",
        "\n",
        "Prophet (Facebook’s model for seasonality and holidays).\n",
        "\n",
        "State space models (e.g., SARIMA with exogenous variables)."
      ],
      "metadata": {
        "id": "426xjtQZ3xcl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXuobrlO366E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}