{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt2YTW3KqLUCadcTJdiK1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam4988/Assignment/blob/main/Assignmnet_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Explain the concept of forward propagation in a neural network.**"
      ],
      "metadata": {
        "id": "2REAqMPAEY_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward propagation is the process where input data is passed through the neural network layers to generate an output. It involves:\n",
        "\n",
        "Input Layer: Receives the raw input data.\n",
        "\n",
        "Weighted Sum: Each neuron computes\n",
        "z\n",
        "=\n",
        "W\n",
        "⋅\n",
        "X\n",
        "+\n",
        "b\n",
        "z=W⋅X+b, where\n",
        "W\n",
        "W are weights,\n",
        "X\n",
        "X is input, and\n",
        "b\n",
        "b is bias.\n",
        "\n",
        "Activation Function: Applies non-linear transformations (e.g., ReLU, sigmoid) to\n",
        "z\n",
        "z, producing\n",
        "a\n",
        "=\n",
        "activation\n",
        "(\n",
        "z\n",
        ")\n",
        "a=activation(z).\n",
        "\n",
        "Layer-wise Progression: This repeats across hidden layers until the output layer produces the final prediction.\n",
        "Forward propagation transforms inputs into predictions, enabling the network to learn hierarchical representations of the data."
      ],
      "metadata": {
        "id": "rQh7L5bBEcNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. What is the purpose of the activation function in forward propagation?**"
      ],
      "metadata": {
        "id": "mkmCRjjKEovm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions serve two key purposes:\n",
        "\n",
        "Introducing Non-Linearity: Without non-linear activation functions, the network would collapse into a linear model, incapable of learning complex patterns.\n",
        "\n",
        "Controlling Neuron Output: They determine whether a neuron should \"fire\" (e.g., ReLU outputs\n",
        "max\n",
        "⁡\n",
        "(\n",
        "0\n",
        ",\n",
        "z\n",
        ")\n",
        "max(0,z)), enabling selective signal propagation.\n",
        "Common activation functions include ReLU (for hidden layers) and sigmoid/softmax (for classification outputs)."
      ],
      "metadata": {
        "id": "eGWmbBRuE1d3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Describe the steps involved in the backward propagation (backpropagation) algorithm.**"
      ],
      "metadata": {
        "id": "lVWVVd_GEweg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation optimizes network weights by minimizing the loss function. Steps include:\n",
        "\n",
        "Forward Pass: Compute predictions and loss (e.g., Mean Squared Error, Cross-Entropy).\n",
        "\n",
        "Compute Output Error: Calculate the derivative of the loss with respect to the predictions.\n",
        "\n",
        "Backward Pass: Propagate the error backward through the network using the chain rule:\n",
        "\n",
        "Calculate gradients of the loss with respect to weights and biases.\n",
        "\n",
        "Update Weights: Adjust weights using optimization algorithms (e.g., Gradient Descent:\n",
        "W\n",
        "=\n",
        "W\n",
        "−\n",
        "η\n",
        "⋅\n",
        "∂\n",
        "L\n",
        "∂\n",
        "W\n",
        "W=W−η⋅\n",
        "∂W\n",
        "∂L\n",
        "​\n",
        " ).\n",
        "This iteratively refines the model to reduce prediction errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "KRn--UaCFCaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. What is the purpose of the chain rule in backpropagation?**"
      ],
      "metadata": {
        "id": "4H5VYd6eFGX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chain rule from calculus enables efficient computation of gradients in composite functions. In backpropagation:\n",
        "\n",
        "**Gradient Decomposition:** Breaks down the derivative of the loss into layer-wise terms\n",
        "e.g.,\n",
        "\n",
        "(∂L/∂W1=∂L/∂a2⋅∂a2/∂z2⋅∂z2/∂W1)\n",
        "\n",
        "\n",
        "**Efficiency:** Avoids redundant calculations by reusing intermediate derivatives, making training feasible for deep networks."
      ],
      "metadata": {
        "id": "jmQruejbFM93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Implement the forward propagation process for a simple neural network with one hidden layer using NumPy.**"
      ],
      "metadata": {
        "id": "NqVHUJKAFPsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    # Hidden layer computation\n",
        "    z1 = np.dot(W1, X) + b1\n",
        "    a1 = np.maximum(0, z1)  # ReLU activation\n",
        "\n",
        "    # Output layer computation\n",
        "    z2 = np.dot(W2, a1) + b2\n",
        "    a2 = 1 / (1 + np.exp(-z2))  # Sigmoid activation (for binary classification)\n",
        "\n",
        "    return a1, a2\n",
        "\n",
        "# Example usage:\n",
        "input_size = 3\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "\n",
        "X = np.array([0.5, -1.2, 0.3])  # Input vector\n",
        "W1 = np.random.randn(hidden_size, input_size)\n",
        "b1 = np.zeros((hidden_size, 1))\n",
        "W2 = np.random.randn(output_size, hidden_size)\n",
        "b2 = np.zeros((output_size, 1))\n",
        "\n",
        "hidden_output, final_output = forward_propagation(X, W1, b1, W2, b2)\n",
        "print(\"Output:\", final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs22LJ_dGdOi",
        "outputId": "fc3bbdbd-431d-4395-c086-691c7e3df5cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: [[0.13491033 0.37668232 0.5        0.35266374]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "The hidden layer applies ReLU for non-linearity.\n",
        "\n",
        "The output layer uses sigmoid for binary classification probabilities.\n",
        "\n",
        "Weights (W1, W2) and biases (b1, b2) are initialized randomly."
      ],
      "metadata": {
        "id": "5dFZMExNGYkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s9aq9CTDGxuO"
      }
    }
  ]
}